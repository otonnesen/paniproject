# Approach
### Hash function
I opted to use a hashing function that was easy to implement and work with, if a little impractical at scale. The function takes an integer in base 10 and returns its base 36 representation (padded with 0s) as its hash.

This function is particularly nice to work with because it has a simple inverse: take an integer in base 36 and return its base 10 representation. This allows us to index our database with auto-incrementing numbers, and to generate our hashes on-the-fly using the numbers generated by the database.

Some of the drawbacks of using this hash function are the following:
	- We're limited to 36^6 = ~2 billion possible hashes
	- The primary key in our database will be used to generate our hashes, which will make it harder to implement customizable hashes down the road, as that would involve either adding a new table to our database, or manually inserting values into primary key column

An option to alleviate some of these problems would be to index the database using the hashes themselves instead of integers, but this setup works fine for the purposes of this project.

### API
Next, I decided how the microservice's API would look. The service essentially has two inverse functions: take a long URL and return a hash, and take a hash and return the associated URL. Thus the API has the following two endpoints:

- `POST` `/v1/shorten`:
Creates a hash for the specified url and adds a new entry into the links table.

Request: `{ "url": "string" }`

Response: `{ "id": number }`

- `GET` `/v1/hash/{hash}`:
Retrieves the original URL corresponding to the hash.

Response: `{ "url": "string" }`

### Implementation
I used Go to write the hashing web app. I wanted the app to be scalable, so my rough plan was to have a database assign integer IDs that could be used to seed the hashes to avoid conflicts between different hashing containers. At this point, I just used a Go dictionary instead of an actual database to get the app up and running for testing.

### Containerization
I hadn't really used docker at all prior to working on this project, so I expected to spend a lot of time here figuring out how to make docker do what I wanted it to do. Luckily, it didn't take very much fiddling to get this initial program to run in a container.

Now the core functionality of my link-shortening microservice was complete. I decided to use sqlite to replace the Go dictionary since I didn't want to have to set up a database server.

### Scaling
Since the different hashing containers all had to communicate with the same database, I had to remove the sqlite integration from the hashing code and put it in its own container and wrapped it in another Go web server.

I used nginx to handle the load-balancing of the hashing containers, and just left the database web server alone, since figuring out how to scale a sqlite database was not something I wanted to think about.

### Redirecting
I implemented the redirection service in a different program. It just sends the hash it gets in the URL to the hashing service, which returns the original URL, and redirects there.

# Running
The project can be launched with one database container, one hashing container, and one redirection container by running `docker-compose up`.
The service can be launched with an arbitrary number of hashing containers by running `docker-compose up --scale web={n}`, where `{n}` is the desired number of workers.

### Ports, endpoints, etc.
The nginx load-balancer runs on localhost:5000, so hashing API requests will go to localhost:5000/v1/shorten and localhost:5000/v1/hash.

The redirector runs on localhost:5001, so a user would be given a shortened link like `localhost:5001/abcdef`, and visiting that page would redirect them to whatever URL is associated with the hash `abcdef` in the database (if there is one).

The ports that these two services run on can be changed by editing the ports they bind to on the host machine in `docker-compose.yml`.
